name: E2E Testing Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly extended tests at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - integration
        - cli
        - provider
        - performance
        - security
      use_real_api_keys:
        description: 'Use real API keys (requires secrets)'
        required: false
        default: false
        type: boolean
      generate_test_data:
        description: 'Generate fresh test data'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  PYTEST_TIMEOUT: 600

jobs:
  environment-check:
    name: Environment Validation
    runs-on: ubuntu-latest
    outputs:
      python-version: ${{ steps.setup.outputs.python-version }}
      cache-key: ${{ steps.setup.outputs.cache-key }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      id: setup-python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
        ffmpeg -version
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout pytest-xdist psutil
    
    - name: Validate environment
      id: setup
      run: |
        echo "python-version=${{ steps.setup-python.outputs.python-version }}" >> $GITHUB_OUTPUT
        echo "cache-key=${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('**/pyproject.toml') }}" >> $GITHUB_OUTPUT
        
        # Check tools availability
        python --version
        pytest --version
        ffmpeg -version
        
        # Verify project structure
        ls -la src/
        ls -la tests/

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: environment-check
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.environment-check.outputs.cache-key }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout pytest-xdist psutil
    
    - name: Run unit tests
      run: |
        python -m pytest tests/unit/ \
          -v \
          --tb=short \
          --timeout=${{ env.PYTEST_TIMEOUT }} \
          --json-report \
          --json-report-file=unit_test_report.json \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          -n auto
      env:
        TEST_MODE: true
        DEEPGRAM_API_KEY: test_key_for_mocking
        ELEVENLABS_API_KEY: test_key_for_mocking
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit
        name: unit-tests
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results
        path: |
          unit_test_report.json
          htmlcov/
          coverage.xml

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [environment-check, unit-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.environment-check.outputs.cache-key }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout psutil
    
    - name: Run integration tests
      run: |
        python -m pytest tests/integration/ \
          -v \
          --tb=short \
          --timeout=${{ env.PYTEST_TIMEOUT }} \
          --json-report \
          --json-report-file=integration_test_report.json \
          --cov=src \
          --cov-report=xml
      env:
        TEST_MODE: true
        DEEPGRAM_API_KEY: test_key_for_mocking
        ELEVENLABS_API_KEY: test_key_for_mocking
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          integration_test_report.json
          coverage.xml

  e2e-cli-tests:
    name: E2E CLI Tests
    runs-on: ubuntu-latest
    needs: [environment-check, integration-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.environment-check.outputs.cache-key }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout psutil
    
    - name: Generate test data
      run: |
        python tests/e2e/test_data_manager.py
    
    - name: Run CLI E2E tests
      run: |
        python -m pytest tests/e2e/test_cli_integration.py \
          -v \
          --tb=short \
          --timeout=900 \
          --json-report \
          --json-report-file=cli_e2e_report.json
      env:
        TEST_MODE: true
        DEEPGRAM_API_KEY: test_key_for_mocking
        ELEVENLABS_API_KEY: test_key_for_mocking
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: cli-e2e-results
        path: |
          cli_e2e_report.json
          tests/e2e/test_data/

  e2e-provider-tests:
    name: E2E Provider Tests
    runs-on: ubuntu-latest
    needs: [environment-check, integration-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.environment-check.outputs.cache-key }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout psutil
    
    - name: Run provider E2E tests
      run: |
        python -m pytest tests/e2e/test_provider_integration.py \
          -v \
          --tb=short \
          --timeout=600 \
          --json-report \
          --json-report-file=provider_e2e_report.json
      env:
        TEST_MODE: true
        DEEPGRAM_API_KEY: test_key_for_mocking
        ELEVENLABS_API_KEY: test_key_for_mocking
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: provider-e2e-results
        path: provider_e2e_report.json

  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: [environment-check, unit-tests]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.environment-check.outputs.cache-key }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout psutil
    
    - name: Run security tests
      run: |
        python -m pytest tests/e2e/test_security.py \
          -v \
          --tb=short \
          --timeout=600 \
          --json-report \
          --json-report-file=security_test_report.json
      env:
        TEST_MODE: true
        DEEPGRAM_API_KEY: test_key_for_mocking
        ELEVENLABS_API_KEY: test_key_for_mocking
    
    - name: Run additional security checks
      run: |
        # Check for common security issues
        python -m pip install bandit safety
        bandit -r src/ -f json -o bandit_report.json || true
        safety check --json --output safety_report.json || true
    
    - name: Upload security results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-results
        path: |
          security_test_report.json
          bandit_report.json
          safety_report.json

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [environment-check, e2e-cli-tests]
    if: github.event_name == 'schedule' || github.event.inputs.test_suite == 'performance' || github.event.inputs.test_suite == 'all'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.environment-check.outputs.cache-key }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout psutil
    
    - name: Generate test data for performance tests
      run: |
        python tests/e2e/test_data_manager.py
    
    - name: Run performance tests
      run: |
        python -m pytest tests/e2e/test_performance.py \
          -v \
          --tb=short \
          --timeout=1800 \
          --json-report \
          --json-report-file=performance_test_report.json
      env:
        TEST_MODE: true
        DEEPGRAM_API_KEY: test_key_for_mocking
        ELEVENLABS_API_KEY: test_key_for_mocking
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: |
          performance_test_report.json
          tests/e2e/test_data/

  comprehensive-e2e:
    name: Comprehensive E2E Suite
    runs-on: ubuntu-latest
    needs: [environment-check, unit-tests, integration-tests]
    if: github.event_name == 'schedule' || github.event.inputs.test_suite == 'all'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.environment-check.outputs.cache-key }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-json-report pytest-timeout psutil
    
    - name: Run comprehensive E2E test suite
      run: |
        python tests/e2e/run_e2e_tests.py \
          --suite all \
          --coverage \
          --verbose \
          --generate-test-data \
          ${{ github.event.inputs.use_real_api_keys == 'true' && '--real-api-keys' || '' }}
      env:
        TEST_MODE: true
        DEEPGRAM_API_KEY: ${{ github.event.inputs.use_real_api_keys == 'true' && secrets.DEEPGRAM_API_KEY || 'test_key_for_mocking' }}
        ELEVENLABS_API_KEY: ${{ github.event.inputs.use_real_api_keys == 'true' && secrets.ELEVENLABS_API_KEY || 'test_key_for_mocking' }}
    
    - name: Upload comprehensive results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: comprehensive-e2e-results
        path: |
          test_results/
          tests/e2e/test_data/

  results-summary:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-cli-tests, e2e-provider-tests, security-tests]
    if: always()
    
    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate summary report
      run: |
        echo "# E2E Test Results Summary" > summary.md
        echo "" >> summary.md
        echo "## Test Execution Status" >> summary.md
        echo "" >> summary.md
        
        # Check results from each job
        echo "| Test Suite | Status | Details |" >> summary.md
        echo "|------------|--------|---------|" >> summary.md
        
        # Unit tests
        if [ "${{ needs.unit-tests.result }}" == "success" ]; then
          echo "| Unit Tests | âœ… PASSED | All unit tests completed successfully |" >> summary.md
        else
          echo "| Unit Tests | âŒ FAILED | Unit tests failed - check logs |" >> summary.md
        fi
        
        # Integration tests
        if [ "${{ needs.integration-tests.result }}" == "success" ]; then
          echo "| Integration Tests | âœ… PASSED | All integration tests completed successfully |" >> summary.md
        else
          echo "| Integration Tests | âŒ FAILED | Integration tests failed - check logs |" >> summary.md
        fi
        
        # CLI E2E tests
        if [ "${{ needs.e2e-cli-tests.result }}" == "success" ]; then
          echo "| CLI E2E Tests | âœ… PASSED | All CLI tests completed successfully |" >> summary.md
        else
          echo "| CLI E2E Tests | âŒ FAILED | CLI E2E tests failed - check logs |" >> summary.md
        fi
        
        # Provider E2E tests
        if [ "${{ needs.e2e-provider-tests.result }}" == "success" ]; then
          echo "| Provider E2E Tests | âœ… PASSED | All provider tests completed successfully |" >> summary.md
        else
          echo "| Provider E2E Tests | âŒ FAILED | Provider E2E tests failed - check logs |" >> summary.md
        fi
        
        # Security tests
        if [ "${{ needs.security-tests.result }}" == "success" ]; then
          echo "| Security Tests | âœ… PASSED | All security tests completed successfully |" >> summary.md
        else
          echo "| Security Tests | âŒ FAILED | Security tests failed - check logs |" >> summary.md
        fi
        
        echo "" >> summary.md
        echo "## Artifacts" >> summary.md
        echo "" >> summary.md
        echo "Test reports and coverage data are available in the workflow artifacts." >> summary.md
        
        cat summary.md
    
    - name: Upload summary
      uses: actions/upload-artifact@v3
      with:
        name: test-results-summary
        path: summary.md
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

  production-readiness-check:
    name: Production Readiness Assessment
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-cli-tests, e2e-provider-tests, security-tests]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Assess production readiness
      run: |
        echo "# Production Readiness Assessment" > readiness.md
        echo "" >> readiness.md
        
        # Calculate overall success rate
        total_jobs=5
        successful_jobs=0
        
        [ "${{ needs.unit-tests.result }}" == "success" ] && ((successful_jobs++))
        [ "${{ needs.integration-tests.result }}" == "success" ] && ((successful_jobs++))
        [ "${{ needs.e2e-cli-tests.result }}" == "success" ] && ((successful_jobs++))
        [ "${{ needs.e2e-provider-tests.result }}" == "success" ] && ((successful_jobs++))
        [ "${{ needs.security-tests.result }}" == "success" ] && ((successful_jobs++))
        
        success_rate=$((successful_jobs * 100 / total_jobs))
        
        echo "**Overall Success Rate: ${success_rate}%**" >> readiness.md
        echo "" >> readiness.md
        
        if [ $success_rate -ge 100 ]; then
          echo "ğŸŸ¢ **PRODUCTION READY** - All critical tests passed" >> readiness.md
          echo "production_ready=true" >> $GITHUB_ENV
        elif [ $success_rate -ge 80 ]; then
          echo "ğŸŸ¡ **NEEDS ATTENTION** - Most tests passed but some issues detected" >> readiness.md
          echo "production_ready=conditional" >> $GITHUB_ENV
        else
          echo "ğŸ”´ **NOT PRODUCTION READY** - Critical test failures detected" >> readiness.md
          echo "production_ready=false" >> $GITHUB_ENV
        fi
        
        echo "" >> readiness.md
        echo "## Recommendations" >> readiness.md
        echo "" >> readiness.md
        
        if [ "${{ needs.unit-tests.result }}" != "success" ]; then
          echo "- âŒ Fix unit test failures before deployment" >> readiness.md
        fi
        
        if [ "${{ needs.security-tests.result }}" != "success" ]; then
          echo "- âŒ Address security test failures immediately" >> readiness.md
        fi
        
        if [ "${{ needs.e2e-cli-tests.result }}" != "success" ]; then
          echo "- âŒ Resolve CLI integration issues" >> readiness.md
        fi
        
        cat readiness.md
    
    - name: Upload readiness assessment
      uses: actions/upload-artifact@v3
      with:
        name: production-readiness-assessment
        path: readiness.md